{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384f8d54-2432-44c5-a986-bd7fbb06a8b6",
   "metadata": {},
   "source": [
    "In this homework, we'll experiment more with Ollama\n",
    "\n",
    "Q1. Running Ollama with Docker\n",
    "\n",
    "Let's run ollama with Docker. We will need to execute the same command as in the lectures:\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "What's the version of ollama client?\n",
    "\n",
    "To find out, enter the container and execute ollama with the -v flag.\n",
    "\n",
    "answer: \\\n",
    "@liveisliiife ➜ /workspaces/llm-zoomcamp (main) $ docker exec -it ollama bash \\\n",
    "root@95a901b51738:/# ollama -v  \\\n",
    "ollama version is 0.3.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca50bc-eeed-473d-8061-c97942727092",
   "metadata": {},
   "source": [
    "Q2. Downloading an LLM\n",
    "\n",
    "We will donwload a smaller LLM - gemma:2b.\n",
    "\n",
    "Again let's enter the container and pull the model:\n",
    "\n",
    "ollama pull gemma:2b \n",
    "\n",
    "In docker, it saved the results into /root/.ollama\n",
    "\n",
    "We're interested in the metadata about this model. You can find it in models/manifests/registry.ollama.ai/library\n",
    "\n",
    "What's the content of the file related to gemma?\n",
    "\n",
    "answer: \\\n",
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":  {\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":   [{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}root@95a901b51738:~/.ollama/models/manifests/registry.ollama.ai/library/gemma# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf0acd8-2b65-4f90-b923-8634c1bffc89",
   "metadata": {},
   "source": [
    "Q3. Running the LLM\n",
    "\n",
    "Test the following prompt: \"10 * 10\". What's the answer?\n",
    "\n",
    "answer: \\\n",
    "/.ollama/models/manifests/registry.ollama.ai/library/gemma# ollama run gemma:2b \\\n",
    "\\>>> 10 * 10 , what is the answer ? \\\n",
    "Sure, the answer is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c4825-0329-455f-8a07-e080c476f2a8",
   "metadata": {},
   "source": [
    "Q4. Donwloading the weights \n",
    "\n",
    "We don't want to pull the weights every time we run a docker container. Let's do it once and have them available every time we start a container.\n",
    "\n",
    "First, we will need to change how we run the container.\n",
    "\n",
    "Instead of mapping the /root/.ollama folder to a named volume, let's map it to a local directory:\n",
    "\n",
    "\n",
    "mkdir ollama_files\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "\n",
    "\n",
    "Now pull the model:\n",
    "\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "\n",
    "What's the size of the ollama_files/models folder?\n",
    "\n",
    "0.6G\n",
    "\n",
    "1.2G\n",
    "\n",
    "1.7G\n",
    "\n",
    "2.2G\n",
    "\n",
    "Hint: on linux, you can use du -h for that.\n",
    "\n",
    "answer:\n",
    "\n",
    "@liveisliiife ➜ /workspaces/llm-zoomcamp/02-Open Source LLMs (main) $ du -h ollama_files\n",
    "\n",
    "1.6G    ollama_files/models/blobs\n",
    "\n",
    "8.0K    ollama_files/models/manifests/registry.ollama.ai/library/gemma\n",
    "\n",
    "12K     ollama_files/models/manifests/registry.ollama.ai/library\n",
    "\n",
    "16K     ollama_files/models/manifests/registry.ollama.ai\n",
    "\n",
    "20K     ollama_files/models/manifests\n",
    "\n",
    "1.6G    ollama_files/models\n",
    "\n",
    "1.6G    ollama_files\n",
    "\n",
    "@liveisliiife ➜ /workspaces/llm-zoomcamp/02-Open Source LLMs (main) $ du -sh ollama_files\n",
    "\n",
    "1.6G    ollama_files\n",
    "\n",
    "so,answer is 1.7G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14761584-81a1-47c7-b4ee-f2d6302960cb",
   "metadata": {},
   "source": [
    "Q5. Adding the weights\n",
    "\n",
    "Let's now stop the container and add the weights to a new image\n",
    "\n",
    "For that, let's create a Dockerfile:\n",
    "\n",
    "FROM ollama/ollama\n",
    "\n",
    "COPY ...\n",
    "\n",
    "What do you put after COPY?\n",
    "\n",
    "answer: ollama_files /root/.ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d62292-4e44-4236-a579-9f9182ef2595",
   "metadata": {},
   "source": [
    "Q6. Serving it\n",
    "\n",
    "Let's build it:\n",
    "\n",
    "docker build -t ollama-gemma2b .\n",
    "\n",
    "And run it:\n",
    "\n",
    "docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
    "We can connect to it using the OpenAI client\n",
    "\n",
    "Let's test it with the following prompt:\n",
    "\n",
    "prompt = \"What's the formula for energy?\"\n",
    "Also, to make results reproducible, set the temperature parameter to 0:\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "How many completion tokens did you get in response?\n",
    "\n",
    "304\n",
    "\n",
    "604\n",
    "\n",
    "904\n",
    "\n",
    "1204\n",
    "\n",
    "answer:\n",
    "\n",
    "@liveisliiife ➜ /workspaces/llm-zoomcamp/02-Open Source LLMs (main) $ ipython \\\n",
    "Python 3.12.1 (main, Aug  8 2024, 18:45:38) [GCC 9.4.0]\n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 8.26.0 -- An enhanced Interactive Python. Type '?' for help.\n",
    "\n",
    "In [1]: ls \\\n",
    "Dockerfile                    hw2.ipynb\n",
    "__pycache__/                  ollama.ipynb\n",
    "docker                        ollama_elasticsearch_docker.ipynb\n",
    "docker-compose.yaml           ollama_files/\n",
    "huggingface_flan_T5.ipynb     prompt.md\n",
    "huggingface_mistral_7b.ipynb  qa_faq.py\n",
    "huggingface_phi3.ipynb        starter.ipynb\n",
    "\n",
    "In [2]: prompt = \"What's the formula for energy?\"\n",
    "\n",
    "In [3]: from openai import OpenAI \\\n",
    "   ...:  \\\n",
    "   ...: client = OpenAI(  \\\n",
    "   ...:        base_url='http://localhost:11434/v1/', \\\n",
    "   ...:        api_key='ollama', \\\n",
    "   ...: ) \n",
    "\n",
    "\n",
    "In [4]: response = client.chat.completions.create( \\\n",
    "   ...:     model=\"gemma:2b\", \\\n",
    "   ...:     messages=[{\"role\":\"user\",\"content\":prompt}], \\\n",
    "   ...:     temperature=0.0 \\\n",
    "   ...: ) \n",
    "\n",
    "\n",
    "In [5]: response \\\n",
    "Out[5]: ChatCompletion(id='chatcmpl-985', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's the formula for energy:\\n\\n**E = K + U**\\n\\nWhere:\\n\\n* **E** is the energy in joules (J)\\n* **K** is the kinetic energy in joules (J)\\n* **U** is the potential energy in joules (J)\\n\\n**Kinetic energy (K)** is the energy an object possesses when it moves or is in motion. It is calculated as half the product of an object's mass (m) and its velocity (v) squared:\\n\\n**K = 1/2 * m * v^2**\\n\\n**Potential energy (U)** is the energy an object possesses when it is in a position or has a specific configuration. It is calculated as the product of an object's mass and the gravitational constant (g) multiplied by the height or distance of the object from a reference point.\\n\\n**Gravitational potential energy (U)** is given by the formula:\\n\\n**U = mgh**\\n\\nWhere:\\n\\n* **m** is the mass of the object in kilograms (kg)\\n* **g** is the acceleration due to gravity in meters per second squared (m/s^2)\\n* **h** is the height or distance of the object in meters (m)\\n\\nThe formula for energy can be used to calculate the total energy of an object, the energy of a specific part of an object, or the change in energy of an object over time.\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724319021, model='gemma:2b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=304, prompt_tokens=34, total_tokens=338))\n",
    "\n",
    "\n",
    "In [6]: total_tokens = response.usage.total_tokens \n",
    "\n",
    "In [7]: total_tokens \\\n",
    "Out[7]: 338 \n",
    "\n",
    "304 is the closest answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda5a33-e7c5-4a09-a129-c32f4e8b2881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
