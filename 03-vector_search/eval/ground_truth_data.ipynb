{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e0fd83-65ea-484e-9f3b-52ba921b52d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49451fe8-2e8e-4269-ad5f-d8888e466820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d18714da-afeb-482a-ace2-16b469d28678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a unique value to each one\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}\"\n",
    "    # combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}-{doc[\"text\"][10:]}\"   # ok\n",
    "    # combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc3bd79d-07b8-407b-807e-66207af38c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc[\"id\"] = generate_document_id(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efde0bce-bb31-42f8-a29b-22db698b42cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp',\n",
       " 'id': '58fa8869'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56b63473-bb87-4307-b5bd-b8cb2c34ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\n"
     ]
    }
   ],
   "source": [
    "unique_id = []\n",
    "\n",
    "for doc in documents:\n",
    "    unique_id.append(doc[\"id\"])\n",
    "\n",
    "print(len(set(unique_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2ed750a-ac7e-4a78-87e5-1ff57bf13927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca3dc12d 2\n",
      "960fb254 2\n",
      "67d2f21c 2\n",
      "297f443c 2\n"
     ]
    }
   ],
   "source": [
    "frequences = {}\n",
    "non_unique = []\n",
    "\n",
    "for element in unique_id:\n",
    "    if element not in frequences:\n",
    "        frequences[element] = 1\n",
    "    else:\n",
    "        frequences[element] += 1\n",
    "\n",
    "for k,v in frequences.items():\n",
    "    if v != 1:\n",
    "        print(k,v)\n",
    "        non_unique.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46d381b6-9d3b-47e9-85eb-7127a6e12192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca3dc12d', '960fb254', '67d2f21c', '297f443c']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0bd6ae3-ecb3-46c2-94b9-5f2f5ca6ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'id': 'ca3dc12d'}\n",
      "*********\n",
      "{'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"', 'section': 'Module 5: pyspark', 'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\", 'course': 'data-engineering-zoomcamp', 'id': 'ca3dc12d'}\n",
      "*********\n",
      "{'text': 'In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\\nSolution description\\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\\nAsia Saeed\\nAlternative Solution:\\nIn my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\\nFor me this works:\\nfeatures = []\\nfor f in dv.feature_names_:\\nstring = f.replace(“=<”, “-le”)\\nfeatures.append(string)\\nPeter Ernicke', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'id': '960fb254'}\n",
      "*********\n",
      "{'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'id': '67d2f21c'}\n",
      "*********\n",
      "{'text': \"They both do the same, it's just less typing from the script.\", 'section': '6. Decision Trees and Ensemble Learning', 'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?', 'course': 'machine-learning-zoomcamp', 'id': '67d2f21c'}\n",
      "*********\n",
      "{'text': 'This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\\nfeatures = [f.replace(\\'=<\\', \\'_\\').replace(\\'=\\', \\'_\\') for f in features]\\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.', 'section': '6. Decision Trees and Ensemble Learning', 'question': 'ValueError: feature_names must be string, and may not contain [, ] or <', 'course': 'machine-learning-zoomcamp', 'id': '960fb254'}\n",
      "*********\n",
      "{'text': 'see here', 'section': '9. Serverless Deep Learning', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'id': '297f443c'}\n",
      "*********\n",
      "{'text': 'As per AWS documentation:\\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\nYou need to do: (change the fields in red)\\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\\nAlternatively you can run the following command without changing anything given you have a default region configured\\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\\nAdded by Humberto Rodriguez', 'section': '10. Kubernetes and TensorFlow Serving', 'question': 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”', 'course': 'machine-learning-zoomcamp', 'id': '297f443c'}\n",
      "*********\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    if doc[\"id\"] in non_unique:\n",
    "        print(doc)\n",
    "        print(\"*********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7e30641-d4f5-4884-8845-46d7a05c8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944 948\n"
     ]
    }
   ],
   "source": [
    "# simple way to check uniqueness by using collections\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "hashes = defaultdict(list)\n",
    "\n",
    "for doc in documents:\n",
    "    doc_id = doc['id']\n",
    "    hashes[doc_id].append(doc)\n",
    "\n",
    "print(len(hashes), len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2818f34-57a4-489d-91eb-0713a331b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca3dc12d 2\n",
      "960fb254 2\n",
      "67d2f21c 2\n",
      "297f443c 2\n"
     ]
    }
   ],
   "source": [
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1a80e77-b2f7-4c5c-9385-7574846d3e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ca3dc12d'},\n",
       " {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  'id': 'ca3dc12d'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes[\"ca3dc12d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9dc9933-c447-4edd-b33f-aa6d62fba1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "947 948\n"
     ]
    }
   ],
   "source": [
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "for doc in documents:\n",
    "    doc[\"id\"] = generate_document_id(doc)\n",
    "\n",
    "hashes = defaultdict(list)\n",
    "\n",
    "for doc in documents:\n",
    "    doc_id = doc['id']\n",
    "    hashes[doc_id].append(doc)\n",
    "\n",
    "print(len(hashes), len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "871c4165-ed63-4e43-8c97-9ec52c15ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593f7569 2\n"
     ]
    }
   ],
   "source": [
    "for k, values in hashes.items():\n",
    "    if len(values) > 1:\n",
    "        print(k, len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73f9f3ab-8e7b-43a1-b6c6-b7e62e1e22e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"They both do the same, it's just less typing from the script.\\nAsked by Andrew Katoch, Added by Edidiong Esu\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'},\n",
       " {'text': \"They both do the same, it's just less typing from the script.\",\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       "  'course': 'machine-learning-zoomcamp',\n",
       "  'id': '593f7569'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes[\"593f7569\"]  # duplicated but will not be problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb875edc-be1a-4800-a890-817f32fc33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('documents-with-ids.json', 'wt') as f_out:\n",
    "    json.dump(documents, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76c7e487-a291-4fe7-ae29-9081ebd47827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\u201cOffice Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon\\u2019t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
      "    \"section\": \"General course-related questions\",\n",
      "    \"question\": \"Course - When will the course start?\",\n",
      "    \"course\": \"data-engineering-zoomcamp\",\n",
      "    \"id\": \"c02e79ef\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\",\n"
     ]
    }
   ],
   "source": [
    "!head documents-with-ids.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64da42ed-312e-4811-834e-068f0084e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating part\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You emulate a student who's taking our course.\n",
    "Formulate 5 questions this student might ask based on a FAQ record. The record\n",
    "should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "section: {section}\n",
    "question: {question}\n",
    "answer: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d575edd1-2b18-45d9-9e38-bc41b19851eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from openai import OpenAI\n",
    "#client = OpenAI()\n",
    "\n",
    "from groq import Groq\n",
    "client = Groq(api_key=\"...\")    # use your api_key\n",
    "\n",
    "\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"llama3-8b-8192\",  # for groq\n",
    "        # model='gpt-4o',         # for openai\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    json_response = response.choices[0].message.content\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "430057b6-4df3-4a91-b86e-3f31a3f6df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89d41851-a944-4a7b-9772-50d2732166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b32bdf-1b24-485f-834e-79b01ca68002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(documents): \n",
    "    doc_id = doc['id']\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "\n",
    "    questions = generate_questions(doc)\n",
    "    results[doc_id] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3083628f-e0cb-4177-9df7-a8fca4e1a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2dd2f-a371-4b7c-abd3-cbcdba382c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.bin', 'rb') as f_in:\n",
    "    results = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63deff7-c665-4e12-ad30-8526947d91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['1f6520ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975d701-eefe-4757-b21e-0d7a97ccbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resulst = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_resulst[doc_id] = json.loads(json_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f04e7a-bccc-4d64-bbec-c6ad8f4d17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = {d['id']: d for d in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2f47d-2f20-4fce-a184-35d41891d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "for doc_id, questions in parsed_resulst.items():\n",
    "    course = doc_index[doc_id]['course']\n",
    "    for q in questions:\n",
    "        final_results.append((q, course, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6114d88-33bf-4cea-98d7-992881bc3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477442ba-aa08-4e6f-b541-38db364c6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_results, columns=['question', 'course', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ff3f8-01bb-4327-9976-12c3e96f05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ground-truth-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21412170-cb84-4cea-b1d4-b09fa9d67f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question,course,document\n",
      "When does the course begin?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I get the course schedule?,data-engineering-zoomcamp,c02e79ef\n",
      "What is the link for course registration?,data-engineering-zoomcamp,c02e79ef\n",
      "How can I receive course announcements?,data-engineering-zoomcamp,c02e79ef\n",
      "Where do I join the Slack channel?,data-engineering-zoomcamp,c02e79ef\n",
      "Where can I find the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "How do I check the prerequisites for this course?,data-engineering-zoomcamp,1f6520ca\n",
      "Where are the course prerequisites listed?,data-engineering-zoomcamp,1f6520ca\n",
      "What are the requirements for joining this course?,data-engineering-zoomcamp,1f6520ca\n"
     ]
    }
   ],
   "source": [
    "!head ground-truth-data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd76e71-b386-46a7-b98e-d4337de30ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
